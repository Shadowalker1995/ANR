╭─xulabzz ~/Dev/NLP/ANR ‹master*›
╰─➤  $ python PyTorchTEST.py -d "Clothing_Shoes_and_Jewelry" -m "VANRA" -e 15 -p 1 -v 50000 -K 5 -h1 10 -h2 50 -c_local 200 -c_global 100 -hiden_size 500 -output_size 50 -rs 1234 -gpu 0 -vb 1 -sm "Clothing_Shoes_and_Jewelry_VANRA"

[utilities.py\select_gpu] os.environ["CUDA_VISIBLE_DEVICES"]: 0

Command: -d Clothing_Shoes_and_Jewelry -m VANRA -e 15 -p 1 -v 50000 -K 5 -h1 10 -h2 50 -c_local 200 -c_global 100 -hiden_size 500 -output_size 50 -rs 1234 -gpu 0 -vb 1 -sm Clothing_Shoes_and_Jewelry_VANRA

Loading 'info' from "./datasets/Clothing_Shoes_and_Jewelry/Clothing_Shoes_and_Jewelry_info.pkl"..
'info' loaded!

[INFO] # of Users: 669,835, # of Items: 335,585

Creating model (Selected Model: VANRA)..
[args.use_cuda: True] Model is on the GPU! (args.gpu: 0, torch.cuda.current_device(): 0)
Model created! Elapsed Time: 9.57s (0.16 minute)

Loading uid_userDoc from "./datasets/Clothing_Shoes_and_Jewelry/Clothing_Shoes_and_Jewelry_uid_userDoc.npy"..
uid_userDoc loaded! [uid_userDoc: (669835, 500)]

Loading iid_itemDoc from "./datasets/Clothing_Shoes_and_Jewelry/Clothing_Shoes_and_Jewelry_iid_itemDoc.npy"..
iid_itemDoc loaded! [iid_itemDoc: (335585, 500)]

Loading pretrained word embeddings from "./datasets/Clothing_Shoes_and_Jewelry/Clothing_Shoes_and_Jewelry_wid_wordEmbed.npy"..
Pretrained word embeddings loaded! [wid_wEmbed: (50002, 300)]

Loading uid_userVis from "./datasets/Clothing_Shoes_and_Jewelry/Clothing_Shoes_and_Jewelry_uid_userVis.npy"..
uid_userVis loaded! [uid_userVis: (669835, 500)]

Loading iid_itemVis from "./datasets/Clothing_Shoes_and_Jewelry/Clothing_Shoes_and_Jewelry_iid_itemVis.npy"..
iid_itemVis loaded! [iid_itemVis: (335585, 500)]

Initialization Complete.. Elapsed Time: 79.16s (1.32 minutes)

Loading training set from "./datasets/Clothing_Shoes_and_Jewelry/Clothing_Shoes_and_Jewelry_split_train.pkl"..
Training set loaded! Note: Training examples are shuffled every epoch, i.e. shuffle = True!

Loading validation set from "./datasets/Clothing_Shoes_and_Jewelry/Clothing_Shoes_and_Jewelry_split_dev.pkl"..
Validation set loaded!

Loading testing set from "./datasets/Clothing_Shoes_and_Jewelry/Clothing_Shoes_and_Jewelry_split_test.pkl"..
Testing set loaded!

Train/Dev/Test splits loaded! |TRAIN|: 800,000, |DEV|: 17,754, |TEST|: 17,846
Train/Dev/Test splits loaded! Elapsed Time: 79.43s (1.32 minutes)

Performing initial evaluation for VALIDATION set..
[Initial] [Dev]  MSE: 23293.62264, MAE: 122.87087

Performing initial evaluation for TESTING set..
[Initial] [Test] MSE: 23322.12068, MAE: 122.62373

Initial Evaluation Complete.. Elapsed Time: 85.70s (1.43 minutes)

Parameters with L2 Regularization (Regularization Strength: 1e-06):
VANRA_RatingPred.uid_userOffset.weight, VANRA_RatingPred.iid_itemOffset.weight

Optimizer: Adam, Loss Function: MSELoss

Model Size: 1,021,722,131
# of Trainable Parameters: 1,301,531
VANRA (
  (uid_userDoc): Embedding(669835, 500), weights = ((669835, 500),), parameters = 334,917,500
  (iid_itemDoc): Embedding(335585, 500), weights = ((335585, 500),), parameters = 167,792,500
  (wid_wEmbed): Embedding(50002, 300), weights = ((50002, 300),), parameters = 15,000,600
  (uid_userVis): Embedding(669835, 500), weights = ((669835, 500),), parameters = 334,917,500
  (iid_itemVis): Embedding(335585, 500), weights = ((335585, 500),), parameters = 167,792,500
  (shared_ANR_ARL): ANR_ARL(
    (aspEmbed): Embedding(5, 30)
    (aspProj): Parameter(5, 300, 10)
  ), weights = ((5, 300, 10), (5, 30)), parameters = 15,150 (Trainable)
  (ANR_AIE): ANR_AIE(  (W_a): Parameter(10, 10)
    (W_u): Parameter(50, 10)
    (w_hu): Parameter(50, 1)
    (W_i): Parameter(50, 10)
    (w_hi): Parameter(50, 1)
  ), weights = ((10, 10), (50, 10), (50, 1), (50, 10), (50, 1)), parameters = 1,200 (Trainable)
  (VANRA_VRL): VANRA_VRL(
    (localAttentionLayer_user): LocalAttention(
      (attention_layer): Sequential(
        (0): Conv2d(1, 1, kernel_size=(3, 1), stride=(1, 1))
        (1): Sigmoid()
      )
      (cnn): Sequential(
        (0): Conv2d(1, 200, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU()
        (2): MaxPool2d(kernel_size=(500, 1), stride=(500, 1), padding=0, dilation=1, ceil_mode=False)
      )
    )
    (globalAttentionLayer_user): GlobalAttention(
      (attention_layer): Sequential(
        (0): Conv2d(1, 1, kernel_size=(500, 1), stride=(1, 1))
        (1): Sigmoid()
      )
      (cnn_1): Sequential(
        (0): Conv2d(1, 100, kernel_size=(2, 1), stride=(1, 1))
        (1): ReLU()
        (2): MaxPool2d(kernel_size=(499, 1), stride=(499, 1), padding=0, dilation=1, ceil_mode=False)
      )
      (cnn_2): Sequential(
        (0): Conv2d(1, 100, kernel_size=(3, 1), stride=(1, 1))
        (1): ReLU()
        (2): MaxPool2d(kernel_size=(498, 1), stride=(498, 1), padding=0, dilation=1, ceil_mode=False)
      )
      (cnn_3): Sequential(
        (0): Conv2d(1, 100, kernel_size=(4, 1), stride=(1, 1))
        (1): ReLU()
        (2): MaxPool2d(kernel_size=(497, 1), stride=(497, 1), padding=0, dilation=1, ceil_mode=False)
      )
    )
    (localAttentionLayer_item): LocalAttention(
      (attention_layer): Sequential(
        (0): Conv2d(1, 1, kernel_size=(3, 1), stride=(1, 1))
        (1): Sigmoid()
      )
      (cnn): Sequential(
        (0): Conv2d(1, 200, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU()
        (2): MaxPool2d(kernel_size=(500, 1), stride=(500, 1), padding=0, dilation=1, ceil_mode=False)
      )
    )
    (globalAttentionLayer_item): GlobalAttention(
      (attention_layer): Sequential(
        (0): Conv2d(1, 1, kernel_size=(500, 1), stride=(1, 1))
        (1): Sigmoid()
      )
      (cnn_1): Sequential(
        (0): Conv2d(1, 100, kernel_size=(2, 1), stride=(1, 1))
        (1): ReLU()
        (2): MaxPool2d(kernel_size=(499, 1), stride=(499, 1), padding=0, dilation=1, ceil_mode=False)
      )
      (cnn_2): Sequential(
        (0): Conv2d(1, 100, kernel_size=(3, 1), stride=(1, 1))
        (1): ReLU()
        (2): MaxPool2d(kernel_size=(498, 1), stride=(498, 1), padding=0, dilation=1, ceil_mode=False)
      )
      (cnn_3): Sequential(
        (0): Conv2d(1, 100, kernel_size=(4, 1), stride=(1, 1))
        (1): ReLU()
        (2): MaxPool2d(kernel_size=(497, 1), stride=(497, 1), padding=0, dilation=1, ceil_mode=False)
      )
    )
    (fcLayer): Sequential(
      (0): Linear(in_features=500, out_features=500, bias=True)
      (1): Dropout(p=0.5, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=500, out_features=50, bias=True)
    )
  ), weights = ((1, 1, 3, 1), (1,), (200, 1, 1, 1), (200,), (1, 1, 500, 1), (1,), (100, 1, 2, 1), (100,), (100, 1, 3, 1), (100,), (100, 1, 4, 1), (100,), (1, 1, 3, 1), (1,), (200, 1, 1, 1), (200,), (1, 1, 500, 1), (1,), (100, 1, 2, 1), (100,), (100, 1, 3, 1), (100,), (100, 1, 4, 1), (100,), (500, 500), (500,), (50, 500), (50,)), parameters = 279,760 (Trainable)
  (VANRA_RatingPred): VANRA_RatingPred(
    (userAspRepDropout): Dropout(p=0.5, inplace=False)
    (itemAspRepDropout): Dropout(p=0.5, inplace=False)
    (uid_userOffset): Embedding(669835, 1)
    (iid_itemOffset): Embedding(335585, 1)
    (globalOffset): Parameter(1,)
  ), weights = ((1,), (669835, 1), (335585, 1)), parameters = 1,005,421 (Trainable)
)


batch_userDoc: torch.Size([128, 500])
batch_itemDoc: torch.Size([128, 500])
batch_userDocEmbed: torch.Size([128, 500, 300])
batch_itemDocEmbed: torch.Size([128, 500, 300])
batch_userVis: torch.Size([128, 500])
batch_itemVis: torch.Size([128, 500])

[Input to ARL] batch_userDocEmbed: torch.Size([128, 500, 300])

============================== Aspect Representation Learning (ARL) ==============================
[Input] batch_docIn: torch.Size([128, 500, 300])

As an example, for <Aspect 0>:

        batch_docIn: torch.Size([128, 500, 300])
        self.aspProj[0]: torch.Size([300, 10])
        batch_aspProjDoc: torch.Size([128, 500, 10])

        batch_aspEmbed: torch.Size([128, 30, 1])

        batch_aspProjDoc_padded [PADDED; Pad Size: 1]: torch.Size([128, 502, 10])
        batch_aspProjDoc_padded: torch.Size([128, 500, 10, 3])
        batch_aspProjDoc_padded: torch.Size([128, 500, 3, 10])
        batch_aspProjDoc_padded: torch.Size([128, 500, 30])

        batch_aspAttn [Window Size: 3]: torch.Size([128, 500, 1])

        batch_aspRep: torch.Size([128, 500, 10])
        batch_aspRep: torch.Size([128, 10])

[Output] <All 5 Aspects>
[Output] batch_aspAttn: torch.Size([128, 5, 500])
[Output] batch_aspRep: torch.Size([128, 5, 10])
============================== ==================================== ==============================

[Output of ARL] userAspAttn: torch.Size([128, 5, 500])
[Output of ARL] userAspDoc:  torch.Size([128, 5, 10])

[Input to ARL] batch_itemDocEmbed: torch.Size([128, 500, 300])

============================== Aspect Representation Learning (ARL) ==============================
[Input] batch_docIn: torch.Size([128, 500, 300])

As an example, for <Aspect 0>:

        batch_docIn: torch.Size([128, 500, 300])
        self.aspProj[0]: torch.Size([300, 10])
        batch_aspProjDoc: torch.Size([128, 500, 10])

        batch_aspEmbed: torch.Size([128, 30, 1])

        batch_aspProjDoc_padded [PADDED; Pad Size: 1]: torch.Size([128, 502, 10])
        batch_aspProjDoc_padded: torch.Size([128, 500, 10, 3])
        batch_aspProjDoc_padded: torch.Size([128, 500, 3, 10])
        batch_aspProjDoc_padded: torch.Size([128, 500, 30])

        batch_aspAttn [Window Size: 3]: torch.Size([128, 500, 1])

        batch_aspRep: torch.Size([128, 500, 10])
        batch_aspRep: torch.Size([128, 10])

[Output] <All 5 Aspects>
[Output] batch_aspAttn: torch.Size([128, 5, 500])
[Output] batch_aspRep: torch.Size([128, 5, 10])
============================== ==================================== ==============================

[Output of ARL] itemAspAttn: torch.Size([128, 5, 500])
[Output of ARL] itemAspDoc:  torch.Size([128, 5, 10])


============================== Aspect Importance Estimation (AIE) ==============================
[Input to AIE] userAspRep: torch.Size([128, 5, 10])
[Input to AIE] itemAspRep: torch.Size([128, 5, 10])

userAspRepTrans: torch.Size([128, 10, 5])
itemAspRepTrans: torch.Size([128, 10, 5])

affinityMatrix: torch.Size([128, 5, 10])
affinityMatrix: torch.Size([128, 5, 5])

userAspImpt: torch.Size([128, 1, 5])
userAspImpt: torch.Size([128, 5, 1])
userAspImpt: torch.Size([128, 5, 1])
userAspImpt: torch.Size([128, 5])

itemAspImpt: torch.Size([128, 1, 5])
itemAspImpt: torch.Size([128, 5, 1])
itemAspImpt: torch.Size([128, 5, 1])
itemAspImpt: torch.Size([128, 5])

[Output of AIE] userAspImpt (i.e. the User Aspect-level Importance): torch.Size([128, 5])
[Output of AIE] itemAspImpt (i.e. the Item Aspect-level Importance): torch.Size([128, 5])
============================== ================================== ==============================


============================== Visual Representation Learning (VRL) ==============================
[Input] batch_userVis: torch.Size([128, 500])
[Input] batch_itemVis: torch.Size([128, 500])

batch_userVis: torch.Size([128, 500, 1])
batch_itemVis: torch.Size([128, 500, 1])

local_user: torch.Size([128, 200, 1, 1])
local_item: torch.Size([128, 200, 1, 1])

global1_user: torch.Size([128, 100, 1, 1])
global2_user: torch.Size([128, 100, 1, 1])
global3_user: torch.Size([128, 100, 1, 1])
global1_item: torch.Size([128, 100, 1, 1])
global2_item: torch.Size([128, 100, 1, 1])
global3_item: torch.Size([128, 100, 1, 1])

out_user: torch.Size([128, 500, 1, 1])
out_item: torch.Size([128, 500, 1, 1])

out_user: torch.Size([128, 500])
out_item: torch.Size([128, 500])

out_user: torch.Size([128, 50])
out_item: torch.Size([128, 50])


**************************************** Aspect-Based Rating Predictor ****************************************
[Input] userAspRep: torch.Size([128, 5, 10])
[Input] itemAspRep: torch.Size([128, 5, 10])
[Input] userAspImpt: torch.Size([128, 5])
[Input] itemAspImpt: torch.Size([128, 5])
[Input] userVisAttn: torch.Size([128, 50])
[Input] itemVisAttn: torch.Size([128, 50])
[Input] batch_uid:  torch.Size([128])
[Input] batch_iid:  torch.Size([128])

batch_userDocOffset: torch.Size([128, 1])
batch_itemDocOffset: torch.Size([128, 1])

[After Dropout (Dropout Rate of 0.5)] userAspRep: torch.Size([128, 5, 10])
[After Dropout (Dropout Rate of 0.5)] itemAspRep: torch.Size([128, 5, 10])

As an example, the aspect-level rating prediction for <Aspect 0>:

        user_Doc: torch.Size([128, 1, 10])
        item_Doc: torch.Size([128, 10, 1])

        aspRating: torch.Size([128, 1, 1])
        aspRating: torch.Size([128, 1])

rating_pred: torch.Size([128, 5]) ('Raw' Aspect-Level Ratings)
rating_pred: torch.Size([128, 5]) (Multiplied with User-Aspect Importance & Item-Aspect Importance)
rating_pred: torch.Size([128, 1]) (Summed over All 5 Aspects)

The rating prediction of Visual Part:

        user_Vis: torch.Size([128, 1, 50])
        item_Vis: torch.Size([128, 50, 1])
rating_pred: torch.Size([128, 1]) (Include Visual Rating Predict)
rating_pred: torch.Size([128, 1]) (Include User & Item Bias)
rating_pred: torch.Size([128, 1]) (Include Global Bias)

[ANR_RatingPred Output] rating_pred: torch.Size([128, 1])
**************************************** ***************************** ****************************************


[Final Output of VANRA] rating_pred: torch.Size([128, 1])

Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6250/6250 [08:23<00:00, 12.41it/s]

[Epoch 1/15] Training Loss: 931.38953   Elapsed Time: 503.81s (0:08:23)
[Epoch 1] [Dev]  MSE: 1.60892, MAE: 0.97349
[Epoch 1] [Test] MSE: 1.61753, MAE: 0.97655

*** MODEL has obtained the best DEV MSE of 1.60892 so far!
*** MODEL saved to "./__saved_models__/Clothing_Shoes_and_Jewelry - VANRA/Clothing_Shoes_and_Jewelry_VANRA_1234.pth"
