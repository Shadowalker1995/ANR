========================================================================================================================
  ARL_lr: 0.01
  ARL_path: 
  batch_size: 128
  channels_global: 100
  channels_local: 200
  command: -d Books -m DAttn -e 10 -p 1 -v 50000 -K 5 -h1 10 -h2 50 -WED 100 -c_local 200 -c_global 100 -hiden_size 500 -output_size 50 -rs 1234 -gpu 0 -vb 1 -sm Books_DAttn
  ctx_win_size: 3
  dataset: Books
  disable_initial_eval: 0
  dropout_rate: 0.5
  epochs: 10
  filters_num: 100
  gpu: 0
  h1: 10
  h2: 50
  hidden_size: 500
  input_dir: ./datasets/Books/
  L2_reg: 1e-06
  learning_rate: 0.002
  loss_function: MSELoss
  max_doc_len: 500
  max_vis_len: 500
  model: DAttn
  num_aspects: 5
  optimizer: Adam
  out_dir: ./experimental_results/Books - DAttn/
  output_size: 50
  pretrained_src: 1
  random_seed: 1234
  save_model: Books_DAttn
  use_cuda: True
  verbose: 1
  vocab_size: 50000
  word_embed_dim: 100
========================================================================================================================

[INFO] # of Users: 621,433, # of Items: 390,310

Creating model (Selected Model: DAttn)..
[args.use_cuda: True] Model is on the GPU! (args.gpu: 0, torch.cuda.current_device(): 0)
Model created! Elapsed Time: 5.23s (0.09 minute)

Loading uid_userDoc from "./datasets/Books/Books_uid_userDoc.npy"..
uid_userDoc loaded! [uid_userDoc: (621433, 500)]

Loading iid_itemDoc from "./datasets/Books/Books_iid_itemDoc.npy"..
iid_itemDoc loaded! [iid_itemDoc: (390310, 500)]

Loading pretrained word embeddings from "./datasets/Books/Books_wed100_wid_wordEmbed.npy"..
Pretrained word embeddings loaded! [wid_wEmbed: (50002, 100)]

Initialization Complete.. Elapsed Time: 41.18s (0.69 minute)

Train/Dev/Test splits loaded! |TRAIN|: 800,000, |DEV|: 20,828, |TEST|: 21,246
Train/Dev/Test splits loaded! Elapsed Time: 41.47s (0.69 minute)

Performing initial evaluation for VALIDATION set..
[Initial] [Dev]  MSE: 17.98484, MAE: 4.10581

Performing initial evaluation for TESTING set..
[Initial] [Test] MSE: 18.13976, MAE: 4.13002

Initial Evaluation Complete.. Elapsed Time: 50.89s (0.85 minute)

Optimizer: Adam, Loss Function: MSELoss

Model Size: 512,756,148
# of Trainable Parameters: 1,884,448
DAttn (
  (uid_userDoc): Embedding(621433, 500), weights = ((621433, 500),), parameters = 310,716,500
  (iid_itemDoc): Embedding(390310, 500), weights = ((390310, 500),), parameters = 195,155,000
  (wid_wEmbed): Embedding(50002, 100), weights = ((50002, 100),), parameters = 5,000,200
  (user_net): Net(
    (localAttentionLayer): LocalAttention(
      (attention_layer): Sequential(
        (0): Conv2d(1, 1, kernel_size=(3, 100), stride=(1, 1), padding=(1, 0))
        (1): Sigmoid()
      )
      (cnn): Sequential(
        (0): Conv2d(1, 200, kernel_size=(1, 100), stride=(1, 1))
        (1): Tanh()
      )
    )
    (globalAttentionLayer): GlobalAttention(
      (attention_layer): Sequential(
        (0): Conv2d(1, 1, kernel_size=(500, 100), stride=(1, 1))
        (1): Sigmoid()
      )
      (convs): ModuleList(
        (0): Conv2d(1, 100, kernel_size=(2, 100), stride=(1, 1))
        (1): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))
        (2): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))
      )
    )
    (fcLayer): Sequential(
      (0): Linear(in_features=500, out_features=500, bias=True)
      (1): Dropout(p=0.5, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=500, out_features=50, bias=True)
    )
    (dropout): Dropout(p=0.5, inplace=False)
  ), weights = ((1, 1, 3, 100), (1,), (200, 1, 1, 100), (200,), (1, 1, 500, 100), (1,), (100, 1, 2, 100), (100,), (100, 1, 3, 100), (100,), (100, 1, 4, 100), (100,), (500, 500), (500,), (50, 500), (50,)), parameters = 436,352 (Trainable)
  (item_net): Net(
    (localAttentionLayer): LocalAttention(
      (attention_layer): Sequential(
        (0): Conv2d(1, 1, kernel_size=(3, 100), stride=(1, 1), padding=(1, 0))
        (1): Sigmoid()
      )
      (cnn): Sequential(
        (0): Conv2d(1, 200, kernel_size=(1, 100), stride=(1, 1))
        (1): Tanh()
      )
    )
    (globalAttentionLayer): GlobalAttention(
      (attention_layer): Sequential(
        (0): Conv2d(1, 1, kernel_size=(500, 100), stride=(1, 1))
        (1): Sigmoid()
      )
      (convs): ModuleList(
        (0): Conv2d(1, 100, kernel_size=(2, 100), stride=(1, 1))
        (1): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))
        (2): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))
      )
    )
    (fcLayer): Sequential(
      (0): Linear(in_features=500, out_features=500, bias=True)
      (1): Dropout(p=0.5, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=500, out_features=50, bias=True)
    )
    (dropout): Dropout(p=0.5, inplace=False)
  ), weights = ((1, 1, 3, 100), (1,), (200, 1, 1, 100), (200,), (1, 1, 500, 100), (1,), (100, 1, 2, 100), (100,), (100, 1, 3, 100), (100,), (100, 1, 4, 100), (100,), (500, 500), (500,), (50, 500), (50,)), parameters = 436,352 (Trainable)
  (DAttn_RatingPred): DAttn_RatingPred(
    (uid_userOffset): Embedding(621433, 1)
    (iid_itemOffset): Embedding(390310, 1)
    (globalOffset): Parameter(1,)
  ), weights = ((1,), (621433, 1), (390310, 1)), parameters = 1,011,744 (Trainable)
)
========================================================================================================================

[Epoch 1/10] Training Loss: 0.89123	Elapsed Time: 595.45s (0:09:55)
[Epoch 1] [Dev]  MSE: 1.08236, MAE: 0.76304
[Epoch 1] [Test] MSE: 1.03834, MAE: 0.74966

*** MODEL has obtained the best DEV MSE of 1.08236 so far!
*** The Best MODEL saved to "./__saved_models__/Books - DAttn/Books_DAttn_1234.pth"

[Epoch 2/10] Training Loss: 0.64932	Elapsed Time: 1,200.18s (0:20:00)
[Epoch 2] [Dev]  MSE: 1.09718, MAE: 0.74655
[Epoch 2] [Test] MSE: 1.05926, MAE: 0.73426

[Epoch 3/10] Training Loss: 0.55010	Elapsed Time: 1,804.86s (0:30:04)
[Epoch 3] [Dev]  MSE: 1.09958, MAE: 0.76301
[Epoch 3] [Test] MSE: 1.06688, MAE: 0.75431

[Epoch 4/10] Training Loss: 0.47783	Elapsed Time: 2,409.61s (0:40:09)
[Epoch 4] [Dev]  MSE: 1.09368, MAE: 0.74966
[Epoch 4] [Test] MSE: 1.06264, MAE: 0.74044

[Epoch 5/10] Training Loss: 0.42167	Elapsed Time: 3,014.35s (0:50:14)
[Epoch 5] [Dev]  MSE: 1.11554, MAE: 0.76026
[Epoch 5] [Test] MSE: 1.08563, MAE: 0.75148

[Epoch 6/10] Training Loss: 0.37607	Elapsed Time: 3,619.10s (1:00:19)
[Epoch 6] [Dev]  MSE: 1.07245, MAE: 0.74424
[Epoch 6] [Test] MSE: 1.04256, MAE: 0.73657

*** MODEL has obtained the best DEV MSE of 1.07245 so far!
*** The Best MODEL saved to "./__saved_models__/Books - DAttn/Books_DAttn_1234.pth"

[Epoch 7/10] Training Loss: 0.33672	Elapsed Time: 4,223.86s (1:10:23)
[Epoch 7] [Dev]  MSE: 1.12722, MAE: 0.75579
[Epoch 7] [Test] MSE: 1.09330, MAE: 0.74667

[Epoch 8/10] Training Loss: 0.30473	Elapsed Time: 4,828.60s (1:20:28)
[Epoch 8] [Dev]  MSE: 1.13416, MAE: 0.75663
[Epoch 8] [Test] MSE: 1.10001, MAE: 0.74737

[Epoch 9/10] Training Loss: 0.27768	Elapsed Time: 5,433.30s (1:30:33)
[Epoch 9] [Dev]  MSE: 1.12911, MAE: 0.76619
[Epoch 9] [Test] MSE: 1.10155, MAE: 0.76061

[Epoch 10/10] Training Loss: 0.25395	Elapsed Time: 6,038.08s (1:40:38)
[Epoch 10] [Dev]  MSE: 1.15455, MAE: 0.77329
[Epoch 10] [Test] MSE: 1.12916, MAE: 0.76783
*** The Last MODEL saved to "./__saved_models__/Books - DAttn/Books_DAttn_1234_10.pth"

[Training Loss]
[0.89123, 0.64932, 0.5501, 0.47783, 0.42167, 0.37607, 0.33672, 0.30473, 0.27768, 0.25395]

[Dev MSE]
[1.08236, 1.09718, 1.09958, 1.09368, 1.11554, 1.07245, 1.12722, 1.13416, 1.12911, 1.15455]
[Test MSE]
[1.03834, 1.05926, 1.06688, 1.06264, 1.08563, 1.04256, 1.0933, 1.10001, 1.10155, 1.12916]
[Test MAE]
[0.74966, 0.73426, 0.75431, 0.74044, 0.75148, 0.73657, 0.74667, 0.74737, 0.76061, 0.76783]


Best Dev MSE: 1.07245 (Obtained during Evaluation #6)
Test MSE: 1.04256, Test MAE: 0.73657

End of Program! Elapsed Time: 6,098.25s (1:41:38)
