╭─xulabzz ~/Dev/NLP/ANR/preprocessing ‹master*›
╰─➤  $ python preprocessing_simple.py -d Books -dev_test_in_train 1

Dataset: Books
[args from argparse.ArgumentParser().parse_args()]
command: preprocessing_simple.py -d Books -dev_test_in_train 1
dataset: Books
dataset_maximum_size: 1000000
dev_test_in_train: True
maxDL: 500
maxVL: 500
minImages: 1
minReviews: 1
minRL: 10
random_seed: 1337
train_ratio: 0.8
vocab: 50000

[INPUT] Source Folder:       ../datasets/
[INPUT] Reviews/Ratings:     ../datasets/reviews_Books.json

[OUTPUT] Category Folder:    ../datasets/Books/
[OUTPUT] env:                ../datasets/Books/Books_env.pkl
[OUTPUT] info:               ../datasets/Books/Books_info.pkl
[OUTPUT] interactions:       ../datasets/Books/Books_interactions.pkl
[OUTPUT] train_interactions: ../datasets/Books/Books_train_interactions.pkl
[OUTPUT] dev_interactions:   ../datasets/Books/Books_dev_interactions.pkl
[OUTPUT] test_interactions:  ../datasets/Books/Books_test_interactions.pkl
[OUTPUT] split_train:        ../datasets/Books/Books_split_train.pkl
[OUTPUT] split_dev:          ../datasets/Books/Books_split_dev.pkl
[OUTPUT] split_test:         ../datasets/Books/Books_split_test.pkl
[OUTPUT] uid_userDoc:        ../datasets/Books/Books_uid_userDoc.npy
[OUTPUT] iid_itemDoc:        ../datasets/Books/Books_iid_itemDoc.npy
[OUTPUT] uid_userDoc:        ../datasets/Books/Books_uid_userVis.npy
[OUTPUT] iid_itemDoc:        ../datasets/Books/Books_iid_itemVis.npy

Preprocessing data for "Books"

[Settings]
Min reviews for user/item: 1
Min review length to qualify as an user-item interaction: 10
Max words for user/item document: 500 (For truncating/padding to get a fixed-size representation)
Top-50000 words in vocabulary being utilized!


Initial pass of reviews to get the user-item interactions!
Initial pass of reviews for "Books": 22507155it [15:00, 25001.28it/s]
[Initial stats] Users: 8,026,324, Items: 2,330,066, Ratings: 22,507,155, Density: 0.0000012


Second pass of visual features to get the item-feature interactions!
Initial pass of reviews for "Books": 100%|███████████████████████████████████████████████████████████████████████| 2303961/2303961 [00:39<00:00, 58319.31it/s]
[Second stats] Items with image: 2,303,961, Images: 2,303,961, Density: 1.0000000


Starting to filter away users & items based on thresold of 1 images!
Updating interactions based on the num of images...
Filtering interactions: 100%|████████████████████████████████████████████████████████████████████████████████| 22507155/22507155 [00:04<00:00, 4868789.48it/s]

Filtered users & items based on thresold of 1 images!
Users: 8026324 -> 7980636, Items: 2330066 -> 2266687
[Current stats] Users: 7980636, Items: 2266687, Ratings: 22310100, Density: 0.0000012
Updating interactions based on the num of images...
Filtering interactions: 100%|████████████████████████████████████████████████████████████████████████████████| 22310100/22310100 [00:04<00:00, 4843824.27it/s]

Filtered users & items based on thresold of 1 images!
Users: 7980636 -> 7980636, Items: 2266687 -> 2266687
[Current stats] Users: 7980636, Items: 2266687, Ratings: 22310100, Density: 0.0000012

No change in # of users or # of items!

[Final stats] Users: 7,980,636, Items: 2,266,687, Ratings: 22,310,100, Density: 0.0000012

Elapsed time for "Books": 1014.64 seconds (16.91 minutes)

Starting to filter away users & items based on thresold of 1 reviews!

Filtered users & items based on thresold of 1 reviews!
Users: 7980636 -> 7980636, Items: 2266687 -> 2266687

No change in # of users or # of items!

[Final stats] Users: 7,980,636, Ites: 2,266,687, Ratings: 22,310,100, Density: 0.0000012

Elapsed time for "Books": 1017.00 seconds (16.95 minutes)


Third pass of reviews to get the rating, date, the num of tokenized review and index!
Third pass of len of reviews for "Books": 22507155it [23:00, 16307.84it/s]
[Current stats] Users: 7,980,636, Items: 2,266,687, Ratings: 22,310,100, Density: 0.0000012

Filtering user-item interactions based on minimum review length of 10 tokens..
Filtering interactions: 100%|████████████████████████████████████████████████████████████████████████████████| 22310100/22310100 [00:03<00:00, 5710931.42it/s]

Filtered users & items based on minimum review length of 10 tokens!
Users: 7,980,636 -> 7,942,222, Items: 2,266,687 -> 2,262,144
[Current stats] Users: 7,942,222, Items: 2,262,144, Ratings: 22,211,827, Density: 0.0000012


Starting to filter away users & items based on thresold of 1 reviews (after removing reviews with <= 10 tokens)!

Filtered users & items based on thresold of 1 reviews!
Users: 7,942,222 -> 7,942,222, Items: 2,262,144 -> 2,262,144

No change in # of users or # of items!

[Final stats] Users: 7,942,222, Items: 2,262,144, Ratings: 22,211,827, Density: 0.0000012

*****************************************************************************************************************************
*** Original Dataset Size (i.e. num_ratings): 22,211,827!
*** Selecting a random subsample of 1,000,000 user-item interactions!
*** Current Dataset Size (i.e. num_ratings):  1,000,000!
*****************************************************************************************************************************
Fourth pass of reviews for "Books": 22507155it [21:49, 17190.98it/s]
interactions:         ../datasets/Books/Books_interactions.pkl



80.0% of ALL reviews are RANDOMLY selected for TRAIN, another 10.0% RANDOMLY selected for DEV, and remaining 10.0% used for TEST.

[Initial Stats] Total Interactions: 1,000,000, TRAIN: 800,000 (80.00%), DEV: 100,000 (10.00%), TEST: 100,000 (10.00%)


Removing users & items who do not appear in the training set, from the dev and test sets..
Updating DEV interactions: 100%|█████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:00<00:00, 1036283.09it/s]
Updating TEST interactions: 100%|████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:00<00:00, 1037539.18it/s]

Removed 79,172 interactions from DEV and 78,754 interactions from TEST! (i.e. Those belonging to Users/Items which do not appear in TRAIN)

[Current Stats] Total Interactions: 842,074, TRAIN: 800,000 (95.00%), DEV: 20,828 (2.47%), TEST: 21,246 (2.52%)


[FINAL Stats] Users: 621,433, Items: 390,310, Ratings: 842,074, Density: 0.0000035

[FINAL Stats] Total Interactions: 842,074, TRAIN: 800,000 (95.00%), DEV: 20,828 (2.47%), TEST: 21,246 (2.52%)

[FINAL Stats][TRAIN] Users: 621,433, Items: 390,310, Ratings: 800,000
[FINAL Stats][DEV]   Users: 17,478, Items: 17,439, Ratings: 20,828
[FINAL Stats][TEST]  Users: 17,725, Items: 17,705, Ratings: 21,246


train_interactions:   ../datasets/Books/Books_train_interactions.pkl
dev_interactions:     ../datasets/Books/Books_dev_interactions.pkl
test_interactions:    ../datasets/Books/Books_test_interactions.pkl

Consolidating user/item reviews from TRAINING set
Consolidating user/item reviews from TRAINING set: 100%|███████████████████████████████████████████████████████████| 800000/800000 [01:10<00:00, 11277.94it/s]

Creating user docs from TRAINING set
Creating item docs from TRAINING set

Minimum User Doc Len: 10, Minimum Item Doc Len: 10

Original number of words (based on USER & ITEM documents constructed from TRAINING set): 404,072
For the vocabulary, we are only using the 50,000 most frequent words
Current number of words: 50,000

For each user doc, converting words to wids using word_wid...: 100%|███████████████████████████████████████████████| 621433/621433 [00:12<00:00, 50766.74it/s]
For each item doc, converting words to wids using word_wid...: 100%|███████████████████████████████████████████████| 390310/390310 [00:16<00:00, 23491.10it/s]
Store the actual length of each user document (before padding): 100%|████████████████████████████████████████████| 621433/621433 [00:00<00:00, 3289471.99it/s]
Store the actual length of each item document (before padding): 100%|████████████████████████████████████████████| 390310/390310 [00:00<00:00, 3325057.57it/s]
Pad the user documents to MAX_DOC_LEN: 100%|██████████████████████████████████████████████████████████████████████| 621433/621433 [00:04<00:00, 136207.25it/s]
Pad the item documents to MAX_DOC_LEN: 100%|███████████████████████████████████████████████████████████████████████| 390310/390310 [00:08<00:00, 44051.10it/s]
Preparing the TRAINING set: 100%|█████████████████████████████████████████████████████████████████████████████████| 800000/800000 [00:01<00:00, 705440.19it/s]
Preparing the DEV set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 20828/20828 [00:00<00:00, 505327.31it/s]
Preparing the TESTING set: 100%|████████████████████████████████████████████████████████████████████████████████████| 21246/21246 [00:00<00:00, 471895.02it/s]
Info:                 ../datasets/Books/Books_info.pkl
Training Set:         ../datasets/Books/Books_split_train.pkl
Validation Set:       ../datasets/Books/Books_split_dev.pkl
Test Set:             ../datasets/Books/Books_split_test.pkl

Creating numpy matrix for uid_userDoc..
User Document Matrix: (621433, 500)
User Document Matrix: ../datasets/Books/Books_uid_userDoc.npy

Creating numpy matrix for iid_itemDoc..
Item Document Matrix: (390310, 500)
Item Document Matrix: ../datasets/Books/Books_iid_itemDoc.npy

Consolidating user/item visual features from TRAINING set
Consolidating user/item visual features from TRAINING set: 100%|███████████████████████████████████████████████████| 800000/800000 [00:33<00:00, 23825.77it/s]

Creating user visuals from TRAINING set
Creating item visuals from TRAINING set

Minimum User Vis Len: 50, Minimum Item Vis Len: 50
Convert user to uid...: 100%|████████████████████████████████████████████████████████████████████████████████████| 621433/621433 [00:00<00:00, 1553447.87it/s]
Convert item to iid...: 100%|█████████████████████████████████████████████████████████████████████████████████████| 390310/390310 [00:00<00:00, 628185.40it/s]
Store the actual length of each user visual feature (before padding): 100%|██████████████████████████████████████| 621433/621433 [00:00<00:00, 3752310.81it/s]
Store the actual length of each item visual feature (before padding): 100%|██████████████████████████████████████| 390310/390310 [00:00<00:00, 3591463.38it/s]
Pad the user visual feature to MAX_VIS_LEN: 100%|██████████████████████████████████████████████████████████████████| 621433/621433 [00:19<00:00, 31872.26it/s]
Pad the item visual feature to MAX_VIS_LEN: 100%|██████████████████████████████████████████████████████████████████| 390310/390310 [00:05<00:00, 73159.27it/s]

Creating numpy matrix for uid_userVis..
User Visual Feature Matrix: (621433, 500)
User Visual Feature Matrix: ../datasets/Books/Books_uid_userDoc.npy

Creating numpy matrix for iid_itemVis..
Item Visual Feature Matrix: (390310, 500)
Item Visual Feature Matrix: ../datasets/Books/Books_iid_itemDoc.npy

Saving all required files for "Books"..
Environment:          ../datasets/Books/Books_env.pkl

All required files for "Books" successfully saved to '../datasets/Books/'

Preprocessing for "Books" done after 7537.00 seconds (125.62 minutes)


Done!!
