========================================================================================================================
  ARL_lr: 0.01
  ARL_path: 
  batch_size: 128
  channels_global: 20
  channels_local: 20
  command: -d Automotive -m DAttn -e 10 -p 1 -v 50000 -K 5 -h1 10 -h2 50 -WED 300 -c_local 20 -c_global 20 -hiden_size 100 -output_size 50 -rs 1234 -gpu 0 -vb 1 -sm Automotive_DAttn
  ctx_win_size: 3
  dataset: Automotive
  disable_initial_eval: 0
  dropout_rate: 0.5
  epochs: 10
  gpu: 0
  h1: 10
  h2: 50
  hidden_size: 100
  input_dir: ./datasets/Automotive/
  L2_reg: 1e-06
  learning_rate: 0.002
  loss_function: MSELoss
  max_doc_len: 500
  max_vis_len: 500
  model: DAttn
  num_aspects: 5
  optimizer: Adam
  out_dir: ./experimental_results/Automotive - DAttn/
  output_size: 50
  pretrained_src: 1
  random_seed: 1234
  save_model: Automotive_DAttn
  use_cuda: True
  verbose: 1
  vocab_size: 50000
  word_embed_dim: 300
========================================================================================================================

[INFO] # of Users: 561,942, # of Items: 233,577

Creating model (Selected Model: DAttn)..
[args.use_cuda: True] Model is on the GPU! (args.gpu: 0, torch.cuda.current_device(): 0)
Model created! Elapsed Time: 4.46s (0.07 minute)

Loading uid_userDoc from "./datasets/Automotive/Automotive_uid_userDoc.npy"..
uid_userDoc loaded! [uid_userDoc: (561942, 500)]

Loading iid_itemDoc from "./datasets/Automotive/Automotive_iid_itemDoc.npy"..
iid_itemDoc loaded! [iid_itemDoc: (233577, 500)]

Loading pretrained word embeddings from "./datasets/Automotive/Automotive_wid_wordEmbed.npy"..
Pretrained word embeddings loaded! [wid_wEmbed: (50002, 300)]

Initialization Complete.. Elapsed Time: 12.23s (0.20 minute)

Train/Dev/Test splits loaded! |TRAIN|: 800,000, |DEV|: 35,546, |TEST|: 35,690
Train/Dev/Test splits loaded! Elapsed Time: 12.50s (0.21 minute)

Performing initial evaluation for VALIDATION set..
[Initial] [Dev]  MSE: 18.55548, MAE: 4.15593

Performing initial evaluation for TESTING set..
[Initial] [Test] MSE: 18.53200, MAE: 4.15088

Initial Evaluation Complete.. Elapsed Time: 49.84s (0.83 minute)

Optimizer: Adam, Loss Function: MSELoss

Model Size: 413,990,734
# of Trainable Parameters: 1,230,634
DAttn (
  (uid_userDoc): Embedding(561942, 500), weights = ((561942, 500),), parameters = 280,971,000
  (iid_itemDoc): Embedding(233577, 500), weights = ((233577, 500),), parameters = 116,788,500
  (wid_wEmbed): Embedding(50002, 300), weights = ((50002, 300),), parameters = 15,000,600
  (DAttn_RatingPred): DAttn_RatingPred(
    (uid_userOffset): Embedding(561942, 1)
    (iid_itemOffset): Embedding(233577, 1)
    (globalOffset): Parameter(1,)
  ), weights = ((1,), (561942, 1), (233577, 1)), parameters = 795,520 (Trainable)
  (localAttentionLayer_user): LocalAttention(
    (attention_layer): Sequential(
      (0): Conv2d(1, 1, kernel_size=(3, 300), stride=(1, 1))
      (1): Sigmoid()
    )
    (cnn): Sequential(
      (0): Conv2d(1, 20, kernel_size=(1, 300), stride=(1, 1))
      (1): Tanh()
      (2): MaxPool2d(kernel_size=(500, 1), stride=(500, 1), padding=0, dilation=1, ceil_mode=False)
    )
  ), weights = ((1, 1, 3, 300), (1,), (20, 1, 1, 300), (20,)), parameters = 6,921 (Trainable)
  (globalAttentionLayer_user): GlobalAttention(
    (attention_layer): Sequential(
      (0): Conv2d(1, 1, kernel_size=(500, 300), stride=(1, 1))
      (1): Sigmoid()
    )
    (cnn_1): Sequential(
      (0): Conv2d(1, 20, kernel_size=(2, 300), stride=(1, 1))
      (1): Tanh()
      (2): MaxPool2d(kernel_size=(499, 1), stride=(499, 1), padding=0, dilation=1, ceil_mode=False)
    )
    (cnn_2): Sequential(
      (0): Conv2d(1, 20, kernel_size=(3, 300), stride=(1, 1))
      (1): Tanh()
      (2): MaxPool2d(kernel_size=(498, 1), stride=(498, 1), padding=0, dilation=1, ceil_mode=False)
    )
    (cnn_3): Sequential(
      (0): Conv2d(1, 20, kernel_size=(4, 300), stride=(1, 1))
      (1): Tanh()
      (2): MaxPool2d(kernel_size=(497, 1), stride=(497, 1), padding=0, dilation=1, ceil_mode=False)
    )
  ), weights = ((1, 1, 500, 300), (1,), (20, 1, 2, 300), (20,), (20, 1, 3, 300), (20,), (20, 1, 4, 300), (20,)), parameters = 204,061 (Trainable)
  (localAttentionLayer_item): LocalAttention(
    (attention_layer): Sequential(
      (0): Conv2d(1, 1, kernel_size=(3, 300), stride=(1, 1))
      (1): Sigmoid()
    )
    (cnn): Sequential(
      (0): Conv2d(1, 20, kernel_size=(1, 300), stride=(1, 1))
      (1): Tanh()
      (2): MaxPool2d(kernel_size=(500, 1), stride=(500, 1), padding=0, dilation=1, ceil_mode=False)
    )
  ), weights = ((1, 1, 3, 300), (1,), (20, 1, 1, 300), (20,)), parameters = 6,921 (Trainable)
  (globalAttentionLayer_item): GlobalAttention(
    (attention_layer): Sequential(
      (0): Conv2d(1, 1, kernel_size=(500, 300), stride=(1, 1))
      (1): Sigmoid()
    )
    (cnn_1): Sequential(
      (0): Conv2d(1, 20, kernel_size=(2, 300), stride=(1, 1))
      (1): Tanh()
      (2): MaxPool2d(kernel_size=(499, 1), stride=(499, 1), padding=0, dilation=1, ceil_mode=False)
    )
    (cnn_2): Sequential(
      (0): Conv2d(1, 20, kernel_size=(3, 300), stride=(1, 1))
      (1): Tanh()
      (2): MaxPool2d(kernel_size=(498, 1), stride=(498, 1), padding=0, dilation=1, ceil_mode=False)
    )
    (cnn_3): Sequential(
      (0): Conv2d(1, 20, kernel_size=(4, 300), stride=(1, 1))
      (1): Tanh()
      (2): MaxPool2d(kernel_size=(497, 1), stride=(497, 1), padding=0, dilation=1, ceil_mode=False)
    )
  ), weights = ((1, 1, 500, 300), (1,), (20, 1, 2, 300), (20,), (20, 1, 3, 300), (20,), (20, 1, 4, 300), (20,)), parameters = 204,061 (Trainable)
  (fcLayer): Sequential (
    (0): Linear(in_features=80, out_features=100, bias=True), weights = ((100, 80), (100,)), parameters = 8,100 (Trainable)
    (1): Dropout(p=0.5, inplace=False), weights = (), parameters = 0
    (2): ReLU(), weights = (), parameters = 0
    (3): Linear(in_features=100, out_features=50, bias=True), weights = ((50, 100), (50,)), parameters = 5,050 (Trainable)
  ), weights = ((100, 80), (100,), (50, 100), (50,)), parameters = 13,150 (Trainable)
)
========================================================================================================================
